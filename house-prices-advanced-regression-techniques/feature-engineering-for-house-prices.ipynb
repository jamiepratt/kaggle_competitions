{
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# My entry for the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition! #\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def clean(df:pd.DataFrame):\n",
    "    df[[\"Exterior1st\", \"Exterior2nd\"]] = df[[\"Exterior1st\", \"Exterior2nd\"]].replace(\n",
    "        {\"Brk Cmn\": \"BrkComm\",\n",
    "         \"BrkCmn\" : \"BrkComm\",\n",
    "         \"Wd Sdng\": \"WdSdng\",\n",
    "         \"Wd Shng\": \"WdShng\"}\n",
    "    )\n",
    "    # Some values of GarageYrBlt are corrupt, so we'll replace them\n",
    "    # with the year the house was built\n",
    "    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    # Read data\n",
    "    data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n",
    "    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n",
    "    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n",
    "\n",
    "    X_train = df_train.copy()\n",
    "    y_train = X_train.pop(\"SalePrice\")\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    perm = np.random.permutation(len(X_train))\n",
    "    X_train = X_train.iloc[perm].reset_index(drop=True)\n",
    "    y_train = y_train.iloc[perm].reset_index(drop=True)\n",
    "\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "# The nominative (unordered) categorical features\n",
    "features_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n",
    "\n",
    "# The ordinal (ordered) categorical features\n",
    "# Pandas calls the categories \"levels\"\n",
    "five_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "ten_levels = list(range(1,11))\n",
    "\n",
    "ordered_levels = {\n",
    "    \"OverallQual\": ten_levels,\n",
    "    \"OverallCond\": ten_levels,\n",
    "    \"ExterQual\": five_levels,\n",
    "    \"ExterCond\": five_levels,\n",
    "    \"BsmtQual\": five_levels,\n",
    "    \"BsmtCond\": five_levels,\n",
    "    \"HeatingQC\": five_levels,\n",
    "    \"KitchenQual\": five_levels,\n",
    "    \"FireplaceQu\": five_levels,\n",
    "    \"GarageQual\": five_levels,\n",
    "    \"GarageCond\": five_levels,\n",
    "    \"PoolQC\": five_levels,\n",
    "    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n",
    "    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "}\n",
    "\n",
    "ordered_levels = {key: [\"None\"] + value for key, value in ordered_levels.items()}\n",
    "\n",
    "\n",
    "def preprocess_pipeline_steps():\n",
    "    imputer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"ordered_cats\", SimpleImputer(strategy=\"constant\", fill_value=\"None\", copy=False), list(ordered_levels.keys())),\n",
    "            (\"nominal_cats\", SimpleImputer(strategy=\"constant\", fill_value=\"None\", copy=False), features_nom)\n",
    "        ],\n",
    "        remainder=SimpleImputer(strategy=\"constant\", fill_value=0.0, copy=False),\n",
    "    )\n",
    "\n",
    "    steps= [(\"clean\", FunctionTransformer(clean, check_inverse=False)),\n",
    "            (\"impute\", imputer),\n",
    "            ('encode', ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"ordered_cats\", OrdinalEncoder(categories=list(ordered_levels.values())),\n",
    "                     list(range(len(ordered_levels.keys())))),\n",
    "                    (\"nominal_cats\", OrdinalEncoder(categories='auto', handle_unknown='use_encoded_value', unknown_value=-1),\n",
    "                     list(range(len(ordered_levels.keys()), len(ordered_levels.keys())+len(features_nom))))\n",
    "                ],\n",
    "                remainder='passthrough',\n",
    "           ))\n",
    "        ]\n",
    "    return steps\n",
    "\n",
    "class PreprocessPipeline(Pipeline):\n",
    "\n",
    "    feature_names_in = None\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        remainder_columns = list(input_features if input_features is not None else self.feature_names_in)\n",
    "        for col in ordered_levels.keys(): remainder_columns.remove(col)\n",
    "        for col in features_nom: remainder_columns.remove(col)\n",
    "\n",
    "        return list(ordered_levels.keys())+features_nom+remainder_columns\n",
    "\n",
    "    def _fit(self, X, y=None, **fit_params_steps):\n",
    "        self.feature_names_in = X.columns\n",
    "        return super()._fit(X, y=None, **fit_params_steps)\n",
    "\n",
    "    def transform(self, X):\n",
    "        fromparent = super().transform(X)\n",
    "        return self.convert_to_df(fromparent)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        fromparent = super().fit_transform(X, y=None, **fit_params)\n",
    "        return self.convert_to_df(fromparent)\n",
    "\n",
    "    def convert_to_df(self, array):\n",
    "        df= pd.DataFrame(array, columns=self.get_feature_names_out())\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(\"float64\")\n",
    "\n",
    "        # Names beginning with numbers are awkward to work with\n",
    "        df = df.rename(columns={\n",
    "                \"1stFlrSF\": \"FirstFlrSF\",\n",
    "                \"2ndFlrSF\": \"SecondFlrSF\",\n",
    "                \"3SsnPorch\": \"Threeseasonporch\",\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "def score_dataset(X, y, estimator):\n",
    "    #Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    log_y = np.log(y)\n",
    "    score = cross_val_score(\n",
    "        estimator, X, log_y, cv=5, scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n",
    "\n",
    "X_train, y_train, X_test = load_data()\n",
    "# # Establish Baseline\n",
    "# # Finally, letâ€™s establish a baseline score to judge our feature engineering against.\n",
    "es = Pipeline(steps=[(\"pre\", PreprocessPipeline(preprocess_pipeline_steps())),\n",
    "                     (\"model\", XGBRegressor())])\n",
    "baseline_score = score_dataset(X_train, y_train, es)\n",
    "print(f\"Baseline score: {baseline_score:.5f} RMSLE\")\n",
    "\n",
    "pp_pipeline = PreprocessPipeline(preprocess_pipeline_steps())\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "X_preproccessed = pp_pipeline.fit_transform(X_train)\n",
    "\n",
    "mi_scores = make_mi_scores(X_preproccessed, y_train)\n",
    "\n",
    "def drop_uninformative(df, col_mi_scores):\n",
    "    return df.loc[:, col_mi_scores > 0.0]\n",
    "\n",
    "fe_pipeline = Pipeline(steps =[(\"du\", FunctionTransformer(drop_uninformative, kw_args={'col_mi_scores':mi_scores}))])\n",
    "\n",
    "pp_fe_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"pp\", pp_pipeline),\n",
    "        (\"fe\", fe_pipeline),\n",
    "        (\"model\", XGBRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformation_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"pp\", pp_pipeline),\n",
    "        (\"fe\", fe_pipeline)\n",
    "    ]\n",
    ")\n",
    "\n",
    "score_dataset(X_train, y_train, pp_fe_pipeline)\n",
    "\n",
    "def mathematical_transforms(df):\n",
    "    X = pd.DataFrame()  # dataframe to hold new features\n",
    "    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n",
    "    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n",
    "    return df.join(X)\n",
    "\n",
    "fe_pipeline.steps.append((\"mt\", FunctionTransformer(mathematical_transforms)))\n",
    "\n",
    "def interactions(df):\n",
    "    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n",
    "    X = X.mul(df.GrLivArea, axis=0)\n",
    "    return df.join(X)\n",
    "\n",
    "fe_pipeline.steps.append((\"int\", FunctionTransformer(interactions)))\n",
    "\n",
    "def counts(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"PorchTypes\"] = df[[\n",
    "        \"WoodDeckSF\",\n",
    "        \"OpenPorchSF\",\n",
    "        \"EnclosedPorch\",\n",
    "        \"ScreenPorch\",\n",
    "    ]].gt(0.0).sum(axis=1)\n",
    "    return df.join(X)\n",
    "\n",
    "fe_pipeline.steps.append((\"ct\", FunctionTransformer(counts)))\n",
    "\n",
    "def group_transforms(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n",
    "    return df.join(X)\n",
    "\n",
    "fe_pipeline.steps.append((\"gt\", FunctionTransformer(group_transforms)))\n",
    "\n",
    "def pca_inspired(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n",
    "    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n",
    "    return df.join(X)\n",
    "\n",
    "fe_pipeline.steps.append((\"pcai\", FunctionTransformer(pca_inspired)))\n",
    "\n",
    "score_dataset(X_train, y_train, pp_fe_pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class CrossFoldEncoder:\n",
    "    def __init__(self, encoder, cols, **kwargs):\n",
    "        self.encoder_ = encoder\n",
    "        self.cols_ = cols\n",
    "        self.kwargs_ = kwargs  # keyword arguments for the encoder\n",
    "        self.cv_ = KFold(n_splits=5)\n",
    "\n",
    "    # Fit an encoder on one split and transform the feature on the\n",
    "    # other. Iterating over the splits in all folds gives a complete\n",
    "    # transformation. We also now have one trained encoder on each\n",
    "    # fold.\n",
    "    def fit(self, X, y):\n",
    "        self.fitted_encoders_ = []\n",
    "        X_encoded = []\n",
    "        for idx_encode, idx_train in self.cv_.split(X):\n",
    "            fitted_encoder = self.encoder_(cols=self.cols_, **self.kwargs_)\n",
    "            fitted_encoder.fit(\n",
    "                X.iloc[idx_encode, :], y.iloc[idx_encode],\n",
    "            )\n",
    "            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[self.cols_])\n",
    "            self.fitted_encoders_.append(fitted_encoder)\n",
    "        X_encoded = pd.concat(X_encoded)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "\n",
    "    # To transform the test data, average the encodings learned from\n",
    "    # each fold.\n",
    "    def transform(self, X):\n",
    "        from functools import reduce\n",
    "\n",
    "        X_encoded_list = []\n",
    "        for fitted_encoder in self.fitted_encoders_:\n",
    "            X_encoded = fitted_encoder.transform(X)\n",
    "            X_encoded_list.append(X_encoded[self.cols_])\n",
    "        X_encoded = reduce(\n",
    "            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n",
    "        ) / len(X_encoded_list)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "\n",
    "cat_boost = (\"catbooster\", CatBoostEncoder(cols=['MSSubClass'], a=1))\n",
    "fe_pipeline.steps.append(cat_boost)"
   ],
   "metadata": {
    "_kg_hide-input": true
   },
   "execution_count": 286,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "score_dataset(X_train, y_train, pp_fe_pipeline)"
   ],
   "metadata": {},
   "execution_count": 287,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1351326790054262"
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning #\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "xgb_params = dict(\n",
    "    max_depth=6,           # maximum depth of each tree - try 2 to 10\n",
    "    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n",
    "    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n",
    "    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n",
    "    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n",
    "    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n",
    "    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n",
    "    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n",
    "    num_parallel_tree=1,   # set > 1 for boosted random forests\n",
    ")\n",
    "pl_params = {'model__'+xgb_params_var_name: val for xgb_params_var_name, val in xgb_params.items()}\n",
    "\n",
    "\n",
    "pp_fe_pipeline.set_params(**pl_params)\n",
    "score_dataset(X_train, y_train, pp_fe_pipeline)"
   ],
   "metadata": {},
   "execution_count": 300,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1251627782781732"
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-04-13 12:57:45,740]\u001B[0m A new study created in memory with name: no-name-ca0d5124-d1b9-415c-902e-21a92f05b0a4\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 12:58:17,210]\u001B[0m Trial 0 finished with value: 2.211172657368156 and parameters: {'max_depth': 10, 'learning_rate': 0.0003945168191265213, 'n_estimators': 4284, 'min_child_weight': 2, 'colsample_bytree': 0.9097327197416958, 'subsample': 0.49877758519981197, 'reg_alpha': 0.014255102852680068, 'reg_lambda': 7.985012472593777}. Best is trial 0 with value: 2.211172657368156.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 12:59:53,709]\u001B[0m Trial 1 finished with value: 0.1262903673814937 and parameters: {'max_depth': 9, 'learning_rate': 0.02068447038417672, 'n_estimators': 3349, 'min_child_weight': 6, 'colsample_bytree': 0.48781061548323934, 'subsample': 0.9885455326050718, 'reg_alpha': 0.00026527781028817896, 'reg_lambda': 0.12960081505619328}. Best is trial 1 with value: 0.1262903673814937.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:00:53,237]\u001B[0m Trial 2 finished with value: 6.309755801771282 and parameters: {'max_depth': 4, 'learning_rate': 0.000194708284474162, 'n_estimators': 3098, 'min_child_weight': 8, 'colsample_bytree': 0.2134358203242438, 'subsample': 0.4805104000600454, 'reg_alpha': 0.0010459691853739745, 'reg_lambda': 0.0010778170513333187}. Best is trial 1 with value: 0.1262903673814937.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:01:35,506]\u001B[0m Trial 3 finished with value: 0.15347629835182866 and parameters: {'max_depth': 7, 'learning_rate': 0.00818958405555288, 'n_estimators': 3675, 'min_child_weight': 10, 'colsample_bytree': 0.3253920844708843, 'subsample': 0.9921725777499362, 'reg_alpha': 14.872993791379077, 'reg_lambda': 46.602441504122666}. Best is trial 1 with value: 0.1262903673814937.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:02:12,652]\u001B[0m Trial 4 finished with value: 0.1269146834587058 and parameters: {'max_depth': 8, 'learning_rate': 0.04549083305799533, 'n_estimators': 1328, 'min_child_weight': 6, 'colsample_bytree': 0.3254267763112136, 'subsample': 0.9677405391597536, 'reg_alpha': 0.036106735170756875, 'reg_lambda': 0.004447827978501014}. Best is trial 1 with value: 0.1262903673814937.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:02:33,893]\u001B[0m Trial 5 finished with value: 0.12584639156260452 and parameters: {'max_depth': 3, 'learning_rate': 0.027074815705828384, 'n_estimators': 1470, 'min_child_weight': 8, 'colsample_bytree': 0.8730797658438303, 'subsample': 0.5991625921303443, 'reg_alpha': 1.4835112986012973, 'reg_lambda': 0.5191854342049101}. Best is trial 5 with value: 0.12584639156260452.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:03:10,688]\u001B[0m Trial 6 finished with value: 1.7708312000639548 and parameters: {'max_depth': 3, 'learning_rate': 0.00045846669091987663, 'n_estimators': 4141, 'min_child_weight': 5, 'colsample_bytree': 0.7360675510671457, 'subsample': 0.6181419203995249, 'reg_alpha': 0.03681031551531297, 'reg_lambda': 2.8787617565023282}. Best is trial 5 with value: 0.12584639156260452.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:05:20,895]\u001B[0m Trial 7 finished with value: 0.12543409547372153 and parameters: {'max_depth': 6, 'learning_rate': 0.017815087462260955, 'n_estimators': 4779, 'min_child_weight': 4, 'colsample_bytree': 0.9868210819210312, 'subsample': 0.6727574226993022, 'reg_alpha': 0.0004894566400578226, 'reg_lambda': 5.193644509456807}. Best is trial 7 with value: 0.12543409547372153.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:07:46,915]\u001B[0m Trial 8 finished with value: 0.13040560893246445 and parameters: {'max_depth': 10, 'learning_rate': 0.03928490044966249, 'n_estimators': 4851, 'min_child_weight': 10, 'colsample_bytree': 0.7507333282225381, 'subsample': 0.2761543760620776, 'reg_alpha': 0.0030011081485217463, 'reg_lambda': 92.35527099061645}. Best is trial 7 with value: 0.12543409547372153.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:08:03,022]\u001B[0m Trial 9 finished with value: 2.554368497631685 and parameters: {'max_depth': 7, 'learning_rate': 0.0006483667177960067, 'n_estimators': 2399, 'min_child_weight': 6, 'colsample_bytree': 0.7500211841199216, 'subsample': 0.21298971596375615, 'reg_alpha': 3.3409381102303173, 'reg_lambda': 5.2598129998882195}. Best is trial 7 with value: 0.12543409547372153.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:10:37,966]\u001B[0m Trial 10 finished with value: 0.12224909069133845 and parameters: {'max_depth': 5, 'learning_rate': 0.002141783966616214, 'n_estimators': 6840, 'min_child_weight': 1, 'colsample_bytree': 0.9997422391109011, 'subsample': 0.7882525746650892, 'reg_alpha': 0.3799230785008865, 'reg_lambda': 0.02506509418423381}. Best is trial 10 with value: 0.12224909069133845.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:13:25,838]\u001B[0m Trial 11 finished with value: 0.1220410741104061 and parameters: {'max_depth': 5, 'learning_rate': 0.0028223777254508435, 'n_estimators': 6958, 'min_child_weight': 1, 'colsample_bytree': 0.9600794798122211, 'subsample': 0.7789486598748186, 'reg_alpha': 0.34267263923249264, 'reg_lambda': 0.012551591979386474}. Best is trial 11 with value: 0.1220410741104061.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:16:09,708]\u001B[0m Trial 12 finished with value: 0.12218277816079219 and parameters: {'max_depth': 5, 'learning_rate': 0.0023109917911514405, 'n_estimators': 7046, 'min_child_weight': 1, 'colsample_bytree': 0.9963235628660136, 'subsample': 0.7844752349002573, 'reg_alpha': 0.3434729146177716, 'reg_lambda': 0.008449893850766794}. Best is trial 11 with value: 0.1220410741104061.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:17:45,417]\u001B[0m Trial 13 finished with value: 0.23857235163272628 and parameters: {'max_depth': 5, 'learning_rate': 0.002436043847432367, 'n_estimators': 7729, 'min_child_weight': 3, 'colsample_bytree': 0.5933230975495265, 'subsample': 0.7965748740821691, 'reg_alpha': 69.44109690544057, 'reg_lambda': 0.0003952707512281171}. Best is trial 11 with value: 0.1220410741104061.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:18:56,286]\u001B[0m Trial 14 finished with value: 0.12390301875598568 and parameters: {'max_depth': 2, 'learning_rate': 0.0061645159717836325, 'n_estimators': 6248, 'min_child_weight': 1, 'colsample_bytree': 0.8580363358929994, 'subsample': 0.8132604430790983, 'reg_alpha': 0.5385883369376688, 'reg_lambda': 0.009959899425108935}. Best is trial 11 with value: 0.1220410741104061.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:21:20,367]\u001B[0m Trial 15 finished with value: 0.1250264066516577 and parameters: {'max_depth': 5, 'learning_rate': 0.0011889129380155843, 'n_estimators': 5879, 'min_child_weight': 3, 'colsample_bytree': 0.6312705567302165, 'subsample': 0.8478577030020924, 'reg_alpha': 0.19403197690335894, 'reg_lambda': 0.07446117937627358}. Best is trial 11 with value: 0.1220410741104061.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:22:51,006]\u001B[0m Trial 16 finished with value: 0.1353060428849639 and parameters: {'max_depth': 6, 'learning_rate': 0.009083822021423275, 'n_estimators': 7851, 'min_child_weight': 2, 'colsample_bytree': 0.8970740705210675, 'subsample': 0.6950071278505712, 'reg_alpha': 4.431675400985801, 'reg_lambda': 0.00010621583022785695}. Best is trial 11 with value: 0.1220410741104061.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:25:11,913]\u001B[0m Trial 17 finished with value: 0.12199099086801907 and parameters: {'max_depth': 4, 'learning_rate': 0.004259286656797621, 'n_estimators': 6927, 'min_child_weight': 1, 'colsample_bytree': 0.8017598422158565, 'subsample': 0.9037555821331148, 'reg_alpha': 0.0064442044964354685, 'reg_lambda': 0.0024178339209708194}. Best is trial 17 with value: 0.12199099086801907.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:26:16,635]\u001B[0m Trial 18 finished with value: 0.12819512508495287 and parameters: {'max_depth': 2, 'learning_rate': 0.09296295648885855, 'n_estimators': 5616, 'min_child_weight': 3, 'colsample_bytree': 0.8041812379866699, 'subsample': 0.8979256041707, 'reg_alpha': 0.004331286906089315, 'reg_lambda': 0.0015338316166119538}. Best is trial 17 with value: 0.12199099086801907.\u001B[0m\n",
      "\u001B[32m[I 2022-04-13 13:28:53,905]\u001B[0m Trial 19 finished with value: 0.12227682514455772 and parameters: {'max_depth': 4, 'learning_rate': 0.004855693937417986, 'n_estimators': 7018, 'min_child_weight': 4, 'colsample_bytree': 0.6448698969233216, 'subsample': 0.9125293751164507, 'reg_alpha': 0.0001231152936822118, 'reg_lambda': 0.2039216463094093}. Best is trial 17 with value: 0.12199099086801907.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'max_depth': 4,\n 'learning_rate': 0.004259286656797621,\n 'n_estimators': 6927,\n 'min_child_weight': 1,\n 'colsample_bytree': 0.8017598422158565,\n 'subsample': 0.9037555821331148,\n 'reg_alpha': 0.0064442044964354685,\n 'reg_lambda': 0.0024178339209708194}"
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    xgb_params = dict(\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n",
    "        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n",
    "        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n",
    "    )\n",
    "    pl_params = {'model__'+xgb_params_var_name: val for xgb_params_var_name, val in xgb_params.items()}\n",
    "\n",
    "    pp_fe_pipeline.set_params(**pl_params)\n",
    "    return score_dataset(X_train, y_train, pp_fe_pipeline)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "pl_params = study.best_params\n",
    "pl_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pl_params = {'model__'+xgb_params_var_name: val for xgb_params_var_name, val in pl_params.items()}\n",
    "pp_fe_pipeline.set_params(**pl_params)\n",
    "# XGB minimizes MSE, but competition loss is RMSLE\n",
    "# So, we need to log-transform y to train and exp-transform the predictions\n",
    "pp_fe_pipeline.fit(X_train, np.log(y_train))\n",
    "predictions = np.exp(pp_fe_pipeline.predict(X_test))\n",
    "\n",
    "output = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ],
   "metadata": {},
   "execution_count": 305,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:36:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"model__colsample_bytree\", \"model__learning_rate\", \"model__max_depth\", \"model__min_child_weight\", \"model__n_estimators\", \"model__reg_alpha\", \"model__reg_lambda\", \"model__subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:36:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"model__colsample_bytree\", \"model__learning_rate\", \"model__max_depth\", \"model__min_child_weight\", \"model__n_estimators\", \"model__reg_alpha\", \"model__reg_lambda\", \"model__subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To submit these predictions to the competition, follow these steps:\n",
    "\n",
    "1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n",
    "2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n",
    "3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
    "4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n",
    "\n",
    "You have now successfully submitted to the competition!\n",
    "\n",
    "# Next Steps #\n",
    "\n",
    "If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n",
    "\n",
    "Be sure to check out [other users' notebooks](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/notebooks) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion), where you can share ideas with other Kagglers.\n",
    "\n",
    "Have fun Kaggling!"
   ],
   "metadata": {}
  }
 ]
}